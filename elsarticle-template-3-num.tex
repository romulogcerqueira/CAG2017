%% This is file `elsarticle-template-3-num.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%%
%% $Id: elsarticle-template-3-num.tex 165 2009-10-08 07:58:10Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-3-num.tex $
%%
%\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
\documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
\usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}
\usepackage{subfigure}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The numcompress package shorten the last page in references.
%% `nodots' option removes dots from firstnames in references.
\usepackage[nodots]{numcompress}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
\usepackage{lineno}

\usepackage[utf8]{inputenc}
\usepackage{caption}

%% Avoids linenumbers to collide with text for 5p format:
\setlength\linenumbersep{3pt}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}


\journal{Computers \& Graphics}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{A Novel GPU-based Sonar Simulation for Real-Time Applications}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}
\author[senai,ufba]{Rômulo Cerqueira} \ead{romulo.cerqueira@ufba.br}
\author[senai]{Tiago Trocoli}
\author[senai]{Gustavo Neves}
\author[ufba]{Luciano Oliveira}
\author[senai]{Sylvain Joyeux}
\author[senai,dfki]{Jan Albiez}

\address[senai]{Brazilian Institute of Robotics, SENAI CIMATEC, Salvador, Bahia, Brazil}
\address[ufba]{Intelligent Vision Research Lab, Federal University of Bahia, Salvador, Bahia, Brazil}
\address[dfki]{Robotics Innovation Center, DFKI GmbH, Bremen, Germany}

\begin{abstract}

Simulating realistic sensor data is essential for tuning algorithms and applications and avoid the risks on real-world trials, according the acquisition characteristics and operation domain. In contrast to other robotic fields, the simulation of underwater robotics systems is more difficult, due to complexity to reproduce realistic models of sensors, hydrodynamic forces acting on the robot and the environments where the operations take place. This paper introduces an innovative underwater imaging sonar simulator by vertex and fragment processing on GPU (Graphics Processing Unit). The virtual scenario is composed of the integration between Gazebo simulator and the Rock (Robot Construction Kit) framework. The OpenSceneGraph 3D frame is processed during shader rendering in order to compute a 3-channel matrix with depth and intensity buffers and angular distortion values, and subsequently fused to build the sonar image. Additional characteristics of imaging sonars such as speckle noise, material properties and object's surface irregularities are also addressed and introduced in sonar frame. To export and display simulation resources, this approach was written in C++ with OpenCV support as Rock packages. The method is evaluated by simulating two kind of imaging sonar devices in different scenarios with huge fidelity and high frame rate. 

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Synthetic Sensor Data \sep Sonar Imaging  \sep GPU-based processing \sep Robot Construction Kit (Rock) \sep Underwater Robotics.

\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
\linenumbers

\section{Introduction}
\label{introduction}

When designing and programming autonomous robotic systems, simulation plays an important role. This applies to physically correct simulations (which are needed to design the hardware but take longer to calculate), as well as to simulations which are not completely physically correct but run in real-time. The latter kind of simulation is important when it comes to developing and testing the control system of autonomous robots, especially the higher level parts. It requires the availability of an applicable simulation platform for rapid prototyping and reproducible virtual environments and sensors to test the decision making algorithms in the control system.

When dealing with autonomous underwater vehicles (AUVs), a real-time simulation plays a key role. Underwater robots usually demand expensive hardware and their target domain can be difficult to access depending on the application. Since an AUV can only scarcely communicate back via mostly unreliable acoustic communication, the robot has to be able to make decisions completely autonomously. While the part dealing with the analysis and interpretation of sensor data can be thoroughly tested on recorded data, for the test and verification of the vehicle's \emph{reaction} to this data, a simulation is needed to reduce the risk of vehicle damage or even vehicle loss in the real world.

Due the AUV acts below the photic zone, with high turbidity and hugh light scattering, the image acquisition by optical devices is limited by short ranges and visibility conditions. Knowing these limitations, the high-frequency sonars systems have been used on navigation and perception applications. Acoustic waves are significantly less affected by water attenuation, facilitating operation at greater ranges even as low to zero visibility conditions with a fast refresh rate. Thus, sonar devices address the main shortcomings of optical sensors though at the expense of providing, in general, noisy data of lower resolution and more difficult interpretation.

In the FlatFish project \cite{albiez2015} was developed an interface to integrate the Gazebo real-time simulator~\footnote{http://gazebosim.org} into the software framework ROCK~\footnote{http://rock-robotics.org/} as presented in \cite{watanabe2015}. With this integration it is able to simulate basic underwater physics and underwater camera systems. The missing part, needed by most underwater robots, was the sonar system.

This paper presents a computationally efficient imaging sonar simulator which manipulates the rendering pipeline to compute a sonar image.

% ----------------------------------------------------------------------------------

\section{Background}
\label{background}

\subsection{Sonar Image Model}
\label{sonar:model}

Sonars are echo-ranging devices that use acoustic energy to locate and survey objects in a desired area underwater. The sonar's transducer emits an acoustic signal (or ping) until they hit with any object or be completely absorbed. When the sound wave collides with a surface, part of this energy is reflected, while other is refracted. Then the sonar data is built by plotting the echo measured back versus time of acoustic signal.

A single beam transmitted from a sonar is seen in Fig. \ref{fig:sonar_geometry}. The horizontal and vertical beamwidths are represented by the azimuth $\psi$ and elevation $\theta$ angles respectively, where each sampling along the beam is named bin. Since the speed of sound underwater is known or can be measured, the time axis effectively corresponds to distance from the device. The backscattered acoustic power in each bin determines the intensity value.

The array of transducer readings, with different azimuth directions, forms the final sonar image. Since all incoming signals converge on the same point, the reflected echoes could have originated anywhere along the corresponding elevation arc at a fixed range. Therefore, the 3D information is lost in the projection into a 2D image \cite{huang2015}.

According to this operation principle, the projection of a 3D point in the sonar image following a nonlinear model depends on the vertical beamwidth of the point. Approximating this elevation angle to the limit (with zero-elevation plane), leads to a linear model in which the sonar can be represented as an orthographic camera \cite{johannsson2010}.

\begin{figure}[h]
    \includegraphics[width=\columnwidth]{figs/sonar_geometry}
    \centering
    \captionsetup{justification=centering}
    \caption{Imaging sonar geometry \cite{huang2015}. By the projection process, all 3D points belong the same elevation arc (represented as dashed red line) will be represented to the same image point in the 2D plane. So the range $r$ and the azimuth angle $\psi$ are measured, however the elevation angle $\theta$ is lost.}
    \label{fig:sonar_geometry}
\end{figure}

% ----------------------------------------------------------------------------------

\subsection{Sonar Characteristics}
\label{sonar:characteristics}

Although the sonar devices address the main shortcomings of optical sensors, they present more difficult data interpretation, such as:

\begin{enumerate}[(a)]
    \item Shadowing: This effect is caused by objects blocking the ultrasonic waves transmission and causing regions behind them without acoustic feedback. These regions are defined by a black spot in the image occluding part of the scene;
    \item Non-uniform resolution: The amount of pixels used to represent an intensity record grow as its range increases;
    \item Changes in viewpoint: Imaging the same scene from different viewpoints can cause occlusions, shadows movements and significant alterations of observable objects \cite{hurtos2014b}. For instance, when an outstanding object is insonified, its shadow gets shortened as the sonar becomes closer to it;
    \item Low SNR (Signal-to-Noise Ratio): The sonar suffers from low SNR mainly due the very-long-range scanning and the presence of speckle noise introduced caused by acoustic wave interferences \cite{abbott1973}.
    % \item Inhomogeneous Insonification: Since the water is a loss energy transmission medium, the farthest bins present less intensity than the closest ones.
\end{enumerate}

% ----------------------------------------------------------------------------------

\subsection{Underwater Sonar Devices}
\label{sonar:devices}

The most common types of acoustic sonars are MSIS (Mechanical Scanning Imaging Sonar) and FLS (Forward-Looking Sonar). In the first one (Fig. \ref{fig:swarths:msis}), with one beam per reading, the sonar image is built for each pulse; these images are usually shown on a display pulse by pulse, and the head position reader is rotated according to motor step angle. After a full $360^{\circ}$ sector reading (or the desired sector defined by left and right limit angles), the accumulated sonar data is overwritten. In contrast, the acquisition of a scan image involves a relatively long time and introduces distortions due the vehicle movement. This sonar device is useful for obstacle avoidance \cite{ganesan2015} and navigation \cite{ribas2010} applications.

For the FLS, as seen in Fig. \ref{fig:swarths:fls}, with \textit{n} beams being read simultaneously, the whole forward view is scanned and the current data is overwritten by the next one with a high framerate, similar to a streaming video imagery for real-time applications. This imaging sonar is commonly used for navigation \cite{fallon2013}, mosaicing \cite{hurtos2014a}, target tracking \cite{liu2016} and 3D reconstruction \cite{huang2015} approaches.

\begin{figure}[h]
    \centering
    \subfigure[][]{
    	\includegraphics[width=0.9\columnwidth]{figs/sonar_swarths_msis}
        \label{fig:swarths:msis}
    }
    \subfigure[][]{
    	\includegraphics[width=0.9\columnwidth]{figs/sonar_swarths_fls}
        \label{fig:swarths:fls}
    }
    \captionsetup{justification=centering}
    \caption{Different underwater sonar readings: Mechanically Scanned Imaging Sonar \subref{fig:swarths:msis} and Forward-Looking Sonar \subref{fig:swarths:fls}.}
    \label{fig:sonar_devices}
\end{figure}

% ----------------------------------------------------------------------------------

\section{Closely-related Works}
\label{relatedworks}

Recent works have been proposed models based on raytracing and tube tracing techniques to simulate sonar data with very accurate results but at a high computational cost. An application of optical ray tracing to the simulation of underwater side-scan sonar imagery was formulated by Bell \cite{bell1997}. The images were generated by the use of acoustic signals represented by rays. The process of projecting rays is repeated for a 2D-array, representing all angles the sonar can emit signal.

Waite \cite{waite2002} applied frequency-domain signal processing to generate synthetic aperture sonar frames. In this method, the acoustic image was created by expressing the Fourier transform of the acoustic pulse used to ensonifying the scene. For 2D forward-looking sonar simulations, Saç et al \cite{sac2015} described his model using optical ray tracing also processed in frequency domain. Gu et al \cite{gu2013} modeled a synthetic forward-looking sonar where the ultrasound beams were formed by a set of rays and the objects by small polygons.

A tube tracing approach was used by Gueriot et al \cite{gueriot2010} to simulate multibeam sonar frames. The wave propagation is presented as a series of rays always orthogonal to the current local wave front. The intersection between a tube and a scene element define a tube footprint. In DeMarco \cite{demarco2015}, the acoustic tubes are used to generate a 3D point cloud of insonified scene before converting it into a sonar frame. Since the material reflectance was statically reflected, it resulted in same intensity values for all points on a single object.

The proposed approach herein entails several novelties. As opposed to the related works, the depth and normal values are directly manipulated during the scene formation, which generate sonar frames with a low computational cost and allow the usage by real-time applications. Also, this method is able to reproduce any type of underwater sonar images, as seen in evaluation tests with two kind of sonar devices.

In addition to our previous work \cite{cerqueira2016}, the normal data can also be defined by bump mapping technique and material's reflectivity. Moreover, the speckle noise is modeled as a non-uniform Gaussian distribution and added to sonar image.

% ----------------------------------------------------------------------------------

\section{GPU-based Sonar Simulation}
\label{dev}

The goal of this work is to simulate any kind of underwater sonar by vertex and fragment processing, with a low computational-time cost. The complete pipeline of this implementation, from the virtual scene to the synthetic acoustic image, is seen in Fig. \ref{fig:sonar_sim} and detailed in the following subsections. The sonar simulation is written in C++ with OpenCV~\footnote{http://opencv.org/} support as Rock~\footnote{http://rock-robotics.org/} packages.

\begin{figure*}[ht]
    \includegraphics[width=0.85\paperwidth]{figs/sonar_sim}
    \centering
    \captionsetup{justification=centering}
    \caption{A graphical representation of the individual steps to get from the OpenSceneGraph scene to a sonar beam data structure.}
    \label{fig:sonar_sim}
\end{figure*}

% ----------------------------------------------------------------------------------

\subsection{Underwater Environment}
\label{dev:uwscene}

The Rock-Gazebo integration \cite{watanabe2015} provides the underwater scenario and allows real-time Hardware-in-the-Loop simulations, where Gazebo handles the physical engines and the Rock's visualization tools are responsible by the scene rendering. The graphical data in Rock are based on OpenSceneGraph~\footnote{http://www.openscenegraph.org/} library, an open source C/C++ 3D graphics toolkit built on OpenGL. The osgOcean~\footnote{http://wiki.ros.org/osgOcean} library is used to simulate the ocean's visual effects, and the ocean buoyancy is defined by the Gazebo plugin as described in Watanabe et al \cite{watanabe2015}.

All scene's aspects, such as world model, robot parts (including sensors and joints) and others objects presented in the environment are defined by SDF files, which uses the SDFormat~\footnote{http://sdformat.org}, a XML format used to describe simulated models and environments for Gazebo. Also, the vehicle and sensor robot description must contain a geometry file. Visual geometries used by the rendering engine are provided in COLLADA format and the collision geometries in STL data.

Each component described in the SDF file becomes a Rock component, which is based on the Orocos RTT (Real Time Toolkit)~\footnote{http://www.orocos.org/rtt} and provides ports, properties and operations as its communication layer. When the models are loaded, Rock-Gazebo creates ports to allow other system components to interact with the simulated models \cite{cerqueira2016}.

% ----------------------------------------------------------------------------------

\subsection{Shader Rendering}
\label{dev:shader}

Shaders run on the graphic card and grant to take full control of the rendering pipeline process. The OpenGL Shading Language (GLSL)~\footnote{https://www.opengl.org/documentation/glsl/} is a high-level language with a C-based syntax which handles the graphics pipeline on the GPU (Graphics Processing Unit). The underwater scene is sampled by a virtual orthographic camera, whose optical axis is aligned with the intended viewing direction of the imaging sonar. By programming custom fragment and vertex shaders, the sonar data is computed as:

\begin{itemize}[(a)]
    \item \textit{Depth} is the camera focal length and is calculated by the euclidean distance to object's surface point;
    \item \textit{Intensity} presents the echo reflection energy based on an object's surface normal;
    \item \textit{Angle distortion} is the angle formed from the camera center column to the camera boundary column, for both directions.
\end{itemize}

These data are normalized in [0,1] interval, where means no energy and maximum echo energy for intensity data respectively. For depth data, the minimum value portrays a close object while the maximum value represents a far one, limited by the sonar maximum range. Angle distortion value is zero in image center column which increases for both borders to present the horizontal fiel-of-view half value.

Most real-world surfaces present irregularities and different reflectances. For more realistic sensing, the normal data can also be defined by bump mapping and material properties. Bump mapping is a perturbation rendering technique to simulate irregularities on the object's surface by passing textures and modifying the normal directions. It is much faster and consumes less resources for the same level of detail compared to displacement mapping, because the geometry remains unchanged. Since bump maps are built in tangent space, interpolating the normal vertex and the texture, a TBN (Tangent, Bitangent and Normal) matrix is used to convert the normal values to world space. The different scenes representation is seen in Fig. \ref{fig:sonar_bump_mapping}.

\begin{figure*}[ht]
    \centering
    \subfigure[][]{
        \includegraphics[width=0.23\paperwidth]{figs/bump_0}
        \label{fig:bump_0}
    }
    \subfigure[][]{
        \includegraphics[width=0.23\paperwidth]{figs/bump_1}
        \label{fig:bump_1}
    }
    \subfigure[][]{
        \includegraphics[width=0.23\paperwidth]{figs/bump_2}
        \label{fig:bump_2}
    }
    \subfigure[][]{
        \includegraphics[width=0.23\paperwidth]{figs/bump_3}
        \label{fig:bump_3}
    }
    \subfigure[][]{
        \includegraphics[width=0.23\paperwidth]{figs/bump_4}
        \label{fig:bump_4}
    }
    \subfigure[][]{
        \includegraphics[width=0.23\paperwidth]{figs/bump_5}
        \label{fig:bump_5}
    }
    \captionsetup{justification=centering}
    \caption{Shader rendering with and without bump mapping processing: sphere without texture \subref{fig:bump_0} and with texture \subref{fig:bump_3}; their respective shader image representation in \subref{fig:bump_1} and \subref{fig:bump_4}, where the blue is the normal channel and green is the depth one; and the final acoustic representation in \subref{fig:bump_2} and \subref{fig:bump_5}. By bump mapping technique, the texture changes the normal directions and the sonar image are more reliable in comparison to real objects appearances.}
    \label{fig:sonar_bump_mapping}
\end{figure*}

Moreover, the reflectance allows to describe properly the intensity back from observable objects in shader processing according their material properties (e.g. aluminium has more reflectance than wood and plastic). If a object present a desired reflectance, its value must be positive. As seen in Fig. \ref{fig:sonar_reflectances}, when the normal values are directly proportional to the reflectance value $R$.

\begin{figure}[h]
    \centering
    \subfigure[][]{
    	\includegraphics[width=0.47\columnwidth]{figs/reflectance_0}
        \label{fig:reflectance:0}
    }
    \subfigure[][]{
    	\includegraphics[width=0.47\columnwidth]{figs/reflectance_0_35}
        \label{fig:reflectance:0.35}
    }
    \subfigure[][]{
    	\includegraphics[width=0.47\columnwidth]{figs/reflectance_1_40}
        \label{fig:reflectance:1.40}
    }
    \subfigure[][]{
    	\includegraphics[width=0.47\columnwidth]{figs/reflectance_2_12}
        \label{fig:reflectance:2.12}
    }
    \captionsetup{justification=centering}
    \caption{Examples of different reflectance values $R$ on shader image representation, where blue is the normal channel and green is the depth channel: raw image \subref{fig:reflectance:0}; $R = 0.35$ \subref{fig:reflectance:0.35}; $R = 1.40$ \subref{fig:reflectance:1.40}; and $R = 2.12$ \subref{fig:reflectance:2.12}.}
    \label{fig:sonar_reflectances}
\end{figure}

At the end, the shader process gives a 3-channel matrix data of intensity, depth and angle distortion stored in each channel.

% ----------------------------------------------------------------------------------

\subsection{Synthetic Sonar Data}
\label{dev:sonardata}

The 3D shader matrix is processed in order to build the corresponding acoustic representation. Since the angular distortion is radially spaced over the horizontal field of view, where all pixels in the same column have the same angle value, the first step is to split the image in number of beam parts. Each column is correlated with its respective beam, according to sonar bearings, as seen in Fig. \ref{fig:sonar_sim} .

Each beam subimage is converted into bin intensities using the depth and intensity channels. In a real imaging sonar, the echo measured back is sampled over time and the bin number is proportional to sensor's range. In other words, the initial bins represent the closest distances, while the latest bins are the furthest ones. Therefore, a depth histogram is evaluated to group the subimage pixels with their respective bins, according to depth channel. This information is used to calculate the accumulated intensity of each bin.

Due to acoustic beam spreading and absorption in the water, the final bins have less echo strength than the first ones, because the energy is lost two-way in the environment. In order to solve it, the sonar uses a time-varying gain for range dependence compensation which spread losses in the bins \cite{urick2013}. In this simulation approach, the accumulated intensity in each bin is normalized as
\begin{equation}
    \label{eq:1}
    I_{bin} = \sum\limits_{x=1}^n \frac{1}{n} \times S(i_{x}) \, ,
\end{equation}
where $I_{bin}$ is the intensity in the bin after the energy normalization, $x$ is the pixel in the shader matrix, $n$ is the depth histogram value (number of pixels) of that bin, $S(i_{x})$ is the sigmoid function and $i_{x}$ is the intensity value of the pixel $x$.

Finally, the sonar image resolution needs to be big enough to fill all bins informations. In this case, the number of bins involved is in direct proportion to the sonar image resolution.

% ----------------------------------------------------------------------------------

\subsection{Noise Model}
\label{dev:noise}

Imaging sonar systems are perturbed by a multiplicative noise known as speckle. It is caused by coherent processing of backscattered signals from multiple distributed targets, that degrades image quality and the visual evaluation. The noisy image has been expressed as \cite{lee1980}:

\begin{equation}
\label{eq:2}
y(t) = x(t) \times n(t) \, ,
\end{equation}

where $t$ is the time instant, $y(t)$ is the noised image, $x(t)$ is the free-noise image and $n(t)$ is the speckle noise matrix. This kind of noise is well-modeled as a Gaussian distribution. The physical explanation is provided by the Central Limit of Theorem, which states that the sum of many independent and identically distributed random variables tends to behave a Gaussian random variable \cite{papoulis2002}.

A Gaussian distribution is built following a non-uniform distribution, skewed towards low values, as seen in Fig. \ref{fig:sonar_sim}, and applied as speckle noise in the simulated sonar image. After that, the simulation sonar data process is done.

\subsection{Rock's Sonar Structure}
\label{dev:rock}

To export and display the sonar image, the simulated data is encapsulated as Rock's sonar data type and provided as an output port of Rock's component.

% ----------------------------------------------------------------------------------

\section{Experimental Results}
\label{results}

In order to evaluate the proposed method, the synthetic images generated by the sonar simulator on different scenarios are presented here. The experiments were performed on a computer with Ubuntu 16.04 64 bits, Intel Core i7 3540M processor running at 3 GHz with 16GB DDR3 RAM memory and Nvidia GF108GLM video card.

The first experiment performed a FLS with the following configuration: field of view of $120^{\circ}$ by $20^{\circ}$; 256 beams simultaneously; 500 bins per each beam; range set at $60m$; and angle tilt between the sonar and AUV at $20^{\circ}$.

% In order to evaluate the proposed method, the synthetic images generated by the underwater sonar simulator are presented here. The virtual scenario consisted of the FlatFish robot, a manifold on the seabed and a grid around the robot, as seen in Figs. \ref{fig:uwscene1} and \ref{fig:uwscene2}, respectively.

% \begin{figure}[!h]
% \includegraphics[width=0.85\columnwidth]{figs/uwscene1}
% \centering
% \captionsetup{justification=centering}
% \caption{FlatFish AUV in ROCK-Gazebo underwater scene.}
% \label{fig:uwscene1}
% \end{figure}
%
% \begin{figure}[!h]
% \includegraphics[width=0.84\columnwidth]{figs/uwscene2}
% \centering
% \captionsetup{justification=centering}
% \caption{The underwater scenario used in the MSIS simulation.}
% \label{fig:uwscene2}
% \end{figure}
%
% The first experiment performed a FLS with the following configuration: field of view of $120^{\circ}$ by $20^{\circ}$; 256 beams simultaneously; 500 bins per each beam; range set at $60m$; and angle tilt between the sonar and AUV at $20^{\circ}$. The manifold model was ensonified to generate the shader matrix from the OpenSceneGraph scene. The frontal face of the target and its shadow, as well as the portion of the seabed, are clearly visible in the FLS image, as seen in Fig. \ref{fig:sim:fls}.
%
% \begin{figure}[!h]
% \includegraphics[width=0.92\columnwidth]{figs/sonar_fls_sim}
% \centering
% \caption{The simulated FLS image.}
% \label{fig:sim:fls}
% \end{figure}
%
% A MSIS on top of the robot was simulated in the second experiment. It was configured as follows: field of view of $3^{\circ}$ by $35^{\circ}$; 500 bins in the single beam; $360^{\circ}$ sector scan reading; range set at $10m$; and a motor step angle of $1.8^{\circ}$. The rotation of the sonar head position produced the synthetic sonar image of the grid surrounding the robot seen in Fig. \ref{fig:sim:msis}.
%
% \begin{figure}[h]
% \includegraphics[width=0.9\columnwidth]{figs/sonar_msis_sim}
% \centering
% \captionsetup{justification=centering}
% \caption{The corresponding simulated MSIS image of surrounding grid.}
% \label{fig:sim:msis}
% \end{figure}

% ----------------------------------------------------------------------------------

\section{Conclusion and Outlook}
\label{}

XXXXXXXXXXXXXXXXXXXXXX

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}          ==>>  [#]
%%   \cite[chap. 2]{key} ==>>  [#, chap. 2]
%%   \citet{key}         ==>>  Author [#]

\section*{Acknowledgment}
%
The authors would like to thank Shell Brazil and ANP for financing the work and SENAI CIMATEC and DFKI RIC for the great institutional support.

%% References with bibTeX database:
\newpage

\nocite{*}
\bibliographystyle{model3-num-names}
\bibliography{elsarticle-template-3-num}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model3-num-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}


\end{document}

%%
%% End of file `elsarticle-template-3-num.tex'.
