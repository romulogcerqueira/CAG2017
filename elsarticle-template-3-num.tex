\documentclass[final,5p,times]{elsarticle}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amssymb}
\usepackage[nodots]{numcompress}
\usepackage{lineno}
\usepackage[utf8]{inputenc}
\usepackage{caption}
\usepackage{hyphenat}
\usepackage{array}
\usepackage{makecell}
\usepackage{todonotes}
\usepackage{xcolor}

\include{myhyph}

%% Avoids linenumbers to collide with text for 5p format:
\setlength\linenumbersep{3pt}


\journal{Computers \& Graphics}

\begin{document}

\begin{frontmatter}


\title{A novel GPU-based sonar simulator for real-time applications}

% \author[senai,ufba]{Rômulo Cerqueira} \ead{romulo.cerqueira@ufba.br}
% \author[senai]{Tiago Trocoli}
% \author[senai]{Gustavo Neves}
% \author[senai]{Sylvain Joyeux}
% \author[senai,dfki]{Jan Albiez}
% \author[ufba]{Luciano Oliveira}

% \address[senai]{Brazilian Institute of Robotics, SENAI CIMATEC, Salvador, Bahia, Brazil}
% \address[ufba]{Intelligent Vision Research Lab, Federal University of Bahia, Salvador, Bahia, Brazil}
% \address[dfki]{Robotics Innovation Center, DFKI GmbH, Bremen, Germany}

\begin{abstract}

\textcolor{blue}{Mainly when applied in the underwater environment, sonar simulation requires great computational effort due to the complexity of acoustic physics. For that, simulation of sonar operation allows to evaluate algorithms and control systems without going to the real underwater environment; that reduces the costs and risks of in-field experiments. This paper tackles with the problem of real-time underwater imaging sonar simulation, by using the OpenGL shading language chain on graphics processing unit. This proposed system is able to simulate two main types of sonar sensors: mechanical scanning imaging sonars and forward-looking sonars. The underwater scenario simulation is performed based on three frameworks: (i) OpenSceneGraph reproduces the ocean visual effects, (ii) Gazebo deals with physics effects, and (iii) the Robot Construction Kit controls the sonar in underwater environments. Our system exploits the rasterization pipeline in order to simulate the sonar devices, which are parameterized with the echo intensity, the distance to the target and the angular distortion, being all calculated over objects shapes in the 3D rendered scene. Sonar-intrinsic speckle noise and object material properties are also considered as part of the acoustic image. Our evaluation demonstrated that the proposed method is able to operate close or faster than the real-world devices' frame rate, as well as generating realistic sonar image quality in different virtual underwater scenarios.}

\end{abstract}

\begin{keyword}
Simulated sensor data
\sep sonar imaging
\sep GPU-based processing
\sep Robot Construction Kit (Rock)
\sep Underwater robotics.

\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
\linenumbers

\section{Introduction}
\label{introduction}

% Importance of simulation for autonomous robots
Simulation is an useful tool for designing and programming autonomous robot
systems. That allows evaluating robot behavior, without dealing with physical
hardware or decision-making algorithms and control systems in real-time
trials, as well as costly and time-consuming field experiments.

When working with autonomous underwater vehicles (AUVs), simulation of
facilities are specially relevant. AUVs usually demand expensive hardware and
perform long-term data gathering operations, taking place in restrictive
sites. As AUV does not need umbilical cable, and the underwater communication
carries on by unreliable acoustic links, the robot should be able to make
completely autonomous decisions, even with low-to-zero external assistance.
While the analysis and interpretation of sensor data can be performed in a
post-processing step, a real-time simulation is strongly needed for testing
and evaluation of vehicle's motion response, avoiding involved risks on
real world rides.

% Importance of imaging sonars
AUVs usually act below the photic zone, with high turbidity and huge light
scattering. This makes the quality of image acquisition by optical devices
limited by a short range, and also artificially illuminated and clear visibility
conditions. To tackle with that limitations, high-frequency sonars have been used
primarily on AUVs' navigation and perception systems. Acoustic waves emitted
by sonars are significantly less affected by water attenuation,
aiding operation at greater ranges even as low-to-zero visibility conditions,
with a fast refresh rate. Although sonar devices usually solve the main
shortcomings of optical sensors, they provide noisy data of lower resolution
and more difficult interpretation.

% Related works
By considering sonar benefits and singularities along with the need to evaluate AUVs,
recent works proposed ray tracing- \cite{bell1997,coiras2009,sac2015,demarco2015,gu2013,kwak2015}
and tube tracing-based \cite{gueriot2010} techniques to simulate acoustic data
with very accurate results, although having a high computational cost.
Bell \cite{bell1997} proposed a simulator based on optical ray tracing for
underwater side-scan sonar imagery; images were generated by acoustic
signals represented by rays, which are repeatedly processed, forming a
2D-array. Coiras and Groen \cite{coiras2009} used frequency-domain
signal processing to produce synthetic aperture sonar frames; in that method,
the acoustic image was created by computing the Fourier transform of the
acoustic pulse used to insonify the scene. For forward-looking sonar
simulations, Saç \textit{et al.}
\cite{sac2015} described a sonar model by computing the ray tracing in
frequency domain; when a ray hits an object in 3D space, three parameters
are calculated to process the acoustic data: the Euclidean distance from
the sonar axis, the intensity of returned signal by Lambert Illumination
model and the surface normal; the reverberation and shadow phenomena are
also considered in the scene rendering. DeMarco \textit{et al.}
\cite{demarco2015} used Gazebo and Robot Operating System (ROS)
\cite{quigley2009} integration to simulate acoustic sound pulses by
ray tracing technique, also producing a 3D point cloud of the \textcolor{blue}{coverage area; the reflected intensity has taken into account the object reflectivity, and the amount of Gaussian and salt-and-pepper noises applied in the sonar image is empirically defined.} Gu \textit{et al} \cite{gu2013} modeled a forward-looking device, where the ultrasound beams were formed by a set of rays; the acoustic image is significantly limited by its representation using only two colors: white, when the ray strikes an object, and black for shadow areas. Kwak \textit{et al.} \cite{kwak2015} improved the previous approach by adding a sound pressure attenuation to produce the gray-scale sonar frame, while the other physical characteristics related to sound transmission are disregarded. Guériot and Sintes \cite{gueriot2010} introduce a volume-based
approach of energy interacting with the scene, and collected by the receiving
sonar; the sound propagation is defined by series of acoustic tubes, being
always orthogonal to the current sonar view, where the reverberation and
objects surface irregularities are also addressed.

\subsection{Contributions}

This paper introduces a novel imaging sonar simulator \textcolor{blue}{that presents some contributions when compared to the existing approaches. Instead of simulating the sound pulse paths and the effects of their hits with the virtual objects, as presented by ray tracing and tube tracing-based methods \cite{bell1997,coiras2009,sac2015,demarco2015,gu2013,kwak2015,gueriot2010},
we take advantage of precomputed geometric data during the rasterization
pipeline to compute the acoustic frame. In addition, all raster data are handled on GPU, accelerating then the simulation process with guarantee of
real-time response, in contrast to the methods found in
\cite{bell1997,coiras2009,sac2015,demarco2015}. Although the models found in \cite{bell1997,coiras2009,sac2015,demarco2015,gu2013,kwak2015,gueriot2010} focused on the simulation of specific sonar device, our model is able to reproduce two kind of sonar devices: mechanical scanning imaging sonar (MSIS)
and forward-looking sonar (FLS).} The intensity measured back from the
insonified objects depends on surface normal directions and reflectivity,
producing more realistic simulated frames than binary representation as
found in \cite{gu2013,kwak2015}. The speckle noise is modeled as a non-uniform Gaussian distribution and applied to the final sonar image, next to real sonar operation, differently to \cite{sac2015,demarco2015,gu2013,kwak2015,gueriot2010}. \textcolor{blue}{On the other hand, the additive noise is considered by the authors in \cite{sac2015,demarco2015}. Finally, we have decided to extend the physical phenomena in the simulator, obeying
the usage in real-time experiments (e.g. decision-making algorithms and control system tuning). Knowing this real-time constraint, the high computational cost phenomena such as reverberation is not included at this point, differently of \cite{sac2015}.}

The main goal here is to build quality and low time\hyp{}consuming acoustic frames, according to underwater sonar image formation and operation modes (see Section \ref{sonar:operation}). \textcolor{blue}{The shader matrix with depth, intensity and angular distortion parameters are extracted from underwater scene during the rasterization pipeline,} and subsequently fused to generate the simulated sonar data, as described in Section \ref{dev}. Qualitative and time evaluation results for two different sonar devices are presented in Section \ref{results}, \textcolor{blue}{allowing the use of the} proposed simulator by real-time applications.

% ----------------------------------------------------------------------------------

\section{Imaging sonar operation}
\label{sonar:operation}

Sonars are echo-ranging devices that use acoustic energy to locate and survey
objects in a desired area. The sonar transducer emits pulses of sound waves
(or ping) until they hit any object or be completely absorbed. When the
acoustic signal collides with a surface, part of this energy is reflected,
while other is refracted. The sonar data is built by plotting the echo measured back versus time of acoustic signal. The transducer reading in a given direction forms a \textit{beam}. A single beam transmitted from a sonar is illustrated in Fig. \ref{fig:sonar_geometry}. The horizontal and vertical beamwidths are represented by the azimuth $\psi$ and elevation $\theta$ angles, respectively, where each sampling along the beam is named as \textit{bin}. The sonar coverage area is defined by $R_{min}$ and $R_{max}$. Since the speed of sound underwater is known, or can be measured, the time delay between the emitted pulses and their echoes reveals how far the objects are (distance $r$), as well as how fast they are moving. The backscattered acoustic power in each bin determines the intensity value.

With different azimuth directions, the array of transducer readings forms the
final sonar image. Since all incoming signals converge to the same point, the
reflected echoes could have been originated anywhere along the corresponding
elevation arc at a fixed range, as depicted in Fig. \ref{fig:sonar_geometry}.
In the acoustic representation, the 3D information is lost in the projection
into a 2D image.

\begin{figure}[t]
    \includegraphics[width=\columnwidth]{figs/sonar_geometry_2}
    \captionsetup{justification=justified}
    \caption{Imaging sonar geometry. By a projection process, all 3D points  belonging to the same elevation arc (represented as dashd orange line) will be represented to the same image point in the 2D plane. Range $r$ and azimuth angle $\psi$ are measured, and elevation angle $\theta$ is lost. Sonar coverage area is defined by $R_{min}$ and $R_{max}$.}
    \label{fig:sonar_geometry}
\end{figure}

% ----------------------------------------------------------------------------------

\subsection{Sonar characteristics}
\label{sonar:characteristics}

Although sonar devices overcome main limitations of optical sensors, they
present more difficult data interpretation due to:

\begin{enumerate}[a)]
    \item \textbf{Shadowing}: This effect is caused by objects blocking the
sound waves transmission and causing regions behind them, without acoustic feedback. These regions are defined by a black spot \textcolor{blue}{in the resulting sonar image};
    \item \textbf{Non-uniform resolution}: The amount of pixels used to represent an intensity record in the Cartesian coordinate system grows as    its range increases. This fact causes image distortions and object flatness;
    \item \textbf{Changes in viewpoint}: Imaging the same scene from different viewpoints can cause occlusions, shadows movements and significant  changes of observable objects \cite{hurtos2014}. For instance, when an    outstanding object is insonified, its shadow is shorter, as the sonar becomes closer;
    \item \textbf{Low signal-to-noise ratio (SNR)}: sonars suffer from low SNR mainly due the very-long-range scanning, and the presence of speckle noise introduced by acoustic wave interferences \cite{abbott1979};
    \item \textbf{Reverberation}: This phenomenon is caused when multiple acoustic waves, returning from the same object, are detected over the same ping, producing duplicated objects.
\end{enumerate}

% ----------------------------------------------------------------------------------
\begin{figure*}[t]
    \centering
    \subfigure[][]{
    	\includegraphics[width=\columnwidth]{figs/sonar_swarths_msis}
        \label{fig:swarths:msis}
    }
    \subfigure[][]{
    	\includegraphics[width=\columnwidth]{figs/sonar_swarths_fls}
        \label{fig:swarths:fls}
    }
    \caption{Different underwater sonar readings: \subref{fig:swarths:msis}
    From a mechanical scanning imaging sonar and \subref{fig:swarths:fls}
    from a forward-looking sonar.}
    \captionsetup{justification=justified}
    \label{fig:sonar_devices}
\end{figure*}

\subsection{Types of underwater sonar devices}
\label{sonar:devices}

The most common types of underwater acoustic sonars are MSIS and FLS. In
the former, the sonar image is built for each pulse, with one beam per
reading (see Fig. \ref{fig:swarths:msis}); the resulting sonar images in MSIS are usually depicted on a display pulse by pulse, and the head position reader is rotated according to motor step angle. After a full $360^{\circ}$ sector reading (or the desired sector defined by left and right limit angles), the accumulated sonar data is overwritten. The acquisition of a scanning image involves a relatively long time, introducing distortions caused by the vehicle movements. This sonar device is generally applied in obstacle avoidance \cite{ganesan2015} and navigation \cite{ribas2010} applications. As illustrated in Fig. \ref{fig:swarths:fls}, the whole forward view of an FLS is scanned and the current data is overwritten by the next scanning in a high frame rate, with all beams being read simultaneously; this is similar to a streaming video imagery for real-time applications; this imaging sonar is commonly used for navigation \cite{fallon2013}, mosaicing \cite{hurtos2014}, target tracking \cite{liu2016} and 3D reconstruction \cite{huang2015a}.

\begin{figure}[t]
    \includegraphics[width=\columnwidth]{figs/uwscene}
    \centering
    \captionsetup{justification=justified}
    \caption{The AUV in Rock-Gazebo underwater scene.}
    \label{fig:uwscene}
\end{figure}

% ----------------------------------------------------------------------------------

\begin{figure*}[t]
    \includegraphics[width=0.85\paperwidth]{figs/sonar_sim}
    \centering
    \captionsetup{justification=justified}
    \caption{A graphical overview of the imaging sonar simulation process: (i) a virtual camera, specialized as the sonar device, samples the underwater scene; (ii) three parameters are calculated by shader rendering on GPU: \textcolor{blue}{angular distortion, intensity and depth}; the shader information is split into beam parts, according to the angular distortion values, and the bin depth and the intensity are defined by: (iii) distance histogram and (iv) energy normalization, respectively; (v) the speckle noise is applied to the final sonar data; (vi) and the simulated acoustic data is presented as Rock's data type.}
    \label{fig:sonar_sim}
\end{figure*}

% ----------------------------------------------------------------------------------

\section{GPU-based sonar simulation}
\label{dev}

The goal of our work is to simulate the two types of underwater sonar, discussed in Section \ref{sonar:devices}, by vertex and fragment processing and low computational cost. The complete pipeline of the proposed simulator (from the virtual scene to the simulated acoustic data) is detailed in the following sections. The sonar simulator is written in C++ with OpenCV \cite{bradski2000} support as Rock packages.

% ----------------------------------------------------------------------------------

\subsection{Rendering underwater scene}
\label{dev:uwscene}

\textcolor{blue}{In Rock-Gazebo framework \cite{watanabe2015}, Gazebo handles with physical forces, while Rock's visualization tools are responsible by the scene rendering. The graphical data in Rock are based on OpenSceneGraph framework, an open source C/C++ 3D graphics toolkit built on OpenGL. The osgOcean framework is used to simulate the ocean visual effects, and the ocean buoyancy is defined by the Gazebo, as described in \cite{watanabe2015}. In our case, Rock-Gazebo integration provides the underwater scenario, allowing also real-time hardware-in-the-loop simulation with a virtual AUV.}

All scene aspects, such as world model, robot parts (including sensors and
joints) and other objects presented in the environment are defined by simulation description files (SDF), which use the SDFormat \cite{sdformat2017}, a XML format used to describe simulated models and environments for Gazebo. Visual and collision geometries of vehicle and sensor robot are also described in specific file formats. Each component described in the SDF file becomes a Rock component, which is based on the Orocos real-time toolkit (RTT) \cite{soetens2005}, providing I/O ports, properties and operations as communication layers. When the models are loaded, Rock-Gazebo creates I/O ports to allow real world or simulated system components interacting with the simulated models. A resulting scene sample of this integration is illustrated in Fig. \ref{fig:uwscene}.

% ----------------------------------------------------------------------------------

\subsection{Sonar rendering}
\label{dev:shader}

The rendering pipeline can be customized by defining GPU shaders. A shader is written in OpenGL Shading Language (GLSL) \cite{rost2009}, a high-level language with a C-based syntax which enables more direct control of graphics pipeline, which avoids the use of low-level or hardware-specific languages. Shaders can describe the characteristics of either a vertex or a fragment (a single pixel). Vertex shaders are responsible by transforming the vertex position into a screen position by the rasterizer, generating texture coordinates for texturing, and lighting the vertex to determine each color. The rasterization results in a set of pixels to be processed by fragment shaders, which manipulate their locations, depth and alpha values, and interpolated parameters from the previous stages, such as colors and textures.

\textcolor{blue}{In our work, the underwater scenes are sampled by a virtual camera (frame-by-frame), whose
optical axis is aligned with the intended viewing \textbf{direction}, the coverage \textbf{range} and the \textbf{opening angle} of the simulated sonar device (see Fig. \ref{fig:sonar_sim}(i)). To simulate the sonar imaging by using virtual camera frames, three parameters are computed in fragment and vertex shaders, during the rendering pipeline. This way, we are able to use the precomputed geometric information during the image rasterization process on GPU. The three parameters to simulate the sonar using a virtual camera are, as illustrated in Fig. \ref{fig:sonar_sim}(ii):}

\begin{itemize}[]
    \item \textbf{Distance / Depth} is the camera focal length, calculated by the
    Euclidean distance to the object surface point; %\textcolor{blue}{To
    %avoid precision limitation, we used the native GLSL 32-bit floating-point depth buffer;}
    \item \textbf{Intensity} presents the echo reflection energy based
    on object surface normal angle up to the camera; %%% falar sobre o 8-bit channel.
    \item \textbf{Angular distortion} is the angle formed from the camera
    center column up to the camera boundary column, for both directions. %\textcolor{blue}{As the sonar's
    %opening angle has a limit of 180 degrees, a 8-bit channel suffices to represent the sonar
    %operation.}
\end{itemize}

\textcolor{blue}{By default, the shader encodes the raster data in 8-bit color channels for Red, Green, Blue and Alpha (RGBA). We take this image data structure to store the intensity, depth and angular distortion parameters in RGB channels. The intensity parameter follows the real sonar common representation as 8-bit values; the depth is replaced by the native GLSL 32-bit depth buffer to avoid precision limitation during the distance histogram (see Fig. \ref{fig:sonar_sim}(iii)), described in subsection \ref{dev:sonardata}; as the projection angle range on shader is $0^{\circ}$ to $90^{\circ}$, for both directions, the angular distortion is represented by 8-bit values without loss of meaning. Also, all these three parameters in the shader matrix are normalized into the interval [0,1]. For the intensity parameter, zero means no energy and one means maximum echo energy; for depth, the minimum value denotes a close object, while the maximum value represents a far one, limited by the sonar maximum range; angle distortion is zero in image center column, and increases for both borders to present the half value of horizontal field of view.}

In real world sensing, surfaces usually present irregularities
and different reflectance values. To render that in a virtual scene, the
intensity values can also be defined by \textcolor{blue}{normal maps} (see Fig. \ref{fig:sonar_normal_mapping}) and material property information (see Fig. \ref{fig:sonar_reflectances}). \textcolor{blue}{Normal mapping} is a perturbation rendering technique to simulate wrinkles on the object surface by passing \textcolor{blue}{textures}, modifying
the normal directions on shaders. This approach consumes less computational
resources for the same level of detail, compared with the displacement mapping
technique, because the geometry remains unchanged. Since \textcolor{blue}{normal maps} are built in tangent space, interpolating the normal vertex and the texture, tangent, bi-tangent and normal (TBN) matrices are computed to convert the normal values into the world space. \textcolor{blue}{The differences by applying normal mapping are illustrated in viewing scene (see Figs. \ref{fig:normal_0} and \ref{fig:normal_1}), in the shader representation (see Figs. \ref{fig:normal_2} and \ref{fig:normal_3}) and the final sonar image (see Figs. \ref{fig:normal_4} and \ref{fig:normal_5})}. The reflectance allows to properly describe the intensity received back
from observable objects in shader processing, according their material
properties. For instance, aluminum has more reflectivity than wood and plastic.
When an object has its reflectivity property defined, the reflectance value
$\rho$ is passed to the fragment shader and processed \textcolor{blue}{on GPU. So the final
pixel intensity represents the product of surface normal angle by the reflectance
value $\rho$. The reflectance affects the shader representation (see Figs. \ref{fig:reflectance:1}, \ref{fig:reflectance:0_35}, \ref{fig:reflectance:1_40} and \ref{fig:reflectance:2_12}) and the final sonar image (see Figs. \ref{fig:reflectance:view:1}, \ref{fig:reflectance:view:0_35}, \ref{fig:reflectance:view:1_40} and \ref{fig:reflectance:view:2_12}).
At the end of sonar rendering step, the shader provides a 3-channel matrix composed by intensity, depth and angular
distortion parameters.}

\begin{figure*}[t]
    \centering
    \setcounter{subfigure}{0}
    \subfigure[]{
        \includegraphics[width=0.25\paperwidth]{figs/normal_map_0}
        \label{fig:normal_0}
    }
    \setcounter{subfigure}{2}
    \subfigure[]{
        \includegraphics[width=0.25\paperwidth]{figs/normal_map_1}
        \label{fig:normal_1}
    }
    \setcounter{subfigure}{4}
    \subfigure[]{
        \includegraphics[width=0.25\paperwidth]{figs/normal_map_2}
        \label{fig:normal_2}
    }
    \setcounter{subfigure}{1}
    \subfigure[]{
        \includegraphics[width=0.25\paperwidth]{figs/normal_map_3}
        \label{fig:normal_3}
    }
    \setcounter{subfigure}{3}
    \subfigure[]{
        \includegraphics[width=0.25\paperwidth]{figs/normal_map_4}
        \label{fig:normal_4}
    }
    \setcounter{subfigure}{5}
    \subfigure[]{
        \includegraphics[width=0.25\paperwidth]{figs/normal_map_5}
        \label{fig:normal_5}
    }
    \captionsetup{justification=justified}
    \caption{Example of shader rendering with \textcolor{blue}{normal mapping}:
    A sphere without \subref{fig:normal_0} and with texture
    \subref{fig:normal_3}; respective shader image representations of the spheres
    in \subref{fig:normal_1} and \subref{fig:normal_4}, where the blue area represents the
    intensity parameter, while the green area represents the depth parameter. The final acoustic
    images are depicted in \subref{fig:normal_2} and \subref{fig:normal_5}. By using
    \textcolor{blue}{normal mapping} technique, the textures changes the normal directions,
    and the sonar image details the appearance of object surface, like
    in real world sensing.}
    \label{fig:sonar_normal_mapping}
\end{figure*}

\begin{figure}[!ht]
    \centering
    \setcounter{subfigure}{0}
    \subfigure[]{
    	\includegraphics[width=0.45\columnwidth]{figs/reflectance_shader_1}
        \label{fig:reflectance:1}
    }
    \setcounter{subfigure}{4}
    \subfigure[]{
        \includegraphics[width=0.45\columnwidth]{figs/reflectance_view_1}
        \label{fig:reflectance:view:1}
    }
    \setcounter{subfigure}{1}
    \subfigure[]{
    	\includegraphics[width=0.45\columnwidth]{figs/reflectance_shader_0_35}
        \label{fig:reflectance:0_35}
    }
    \setcounter{subfigure}{5}
    \subfigure[]{
        \includegraphics[width=0.45\columnwidth]{figs/reflectance_view_0_35}
        \label{fig:reflectance:view:0_35}
    }
    \setcounter{subfigure}{2}
    \subfigure[]{
    	\includegraphics[width=0.45\columnwidth]{figs/reflectance_shader_1_40}
        \label{fig:reflectance:1_40}
    }
    \setcounter{subfigure}{6}
    \subfigure[]{
        \includegraphics[width=0.45\columnwidth]{figs/reflectance_view_1_40}
        \label{fig:reflectance:view:1_40}
    }
    \setcounter{subfigure}{3}
    \subfigure[]{
    	\includegraphics[width=0.45\columnwidth]{figs/reflectance_shader_2_12}
        \label{fig:reflectance:2_12}
    }
    \setcounter{subfigure}{7}
    \subfigure[]{
        \includegraphics[width=0.45\columnwidth]{figs/reflectance_view_2_12}
        \label{fig:reflectance:view:2_12}
    }
    \captionsetup{justification=justified}
    \caption{Examples of different reflectance values, $\rho$, applied in
    shader image representation of the same target, where blue is the normal channel and
    green is the depth channel: \subref{fig:reflectance:1} raw image;
    \subref{fig:reflectance:0_35} $\rho = 0.35$;
    \subref{fig:reflectance:1_40} $\rho = 1.40$; and
    \subref{fig:reflectance:2_12} $\rho = 2.12$. \textcolor{blue}{The following
    acoustic images are presented in \subref{fig:reflectance:view:1},
    \subref{fig:reflectance:view:0_35}, \subref{fig:reflectance:view:1_40}
    and \subref{fig:reflectance:view:2_12}.}}
    \label{fig:sonar_reflectances}
\end{figure}


% ----------------------------------------------------------------------------------

\subsection{Simulating sonar device operation}
\label{dev:sonardata}

The \textcolor{blue}{3-channel} shader matrix is used to compute the corresponding
acoustic representation. Since the angular distortion is radially spaced
over the horizontal field-of-view, where all pixels in the same column
have the same angle value, the first step is to split the image into a
number of beam parts. Each angular distortion column is correlated with a respective beam vector,
according to sonar bearings, as illustrated in Fig. \ref{fig:sonar_sim}(vi).
\textcolor{blue}{One beam represents one or more columns.} Each
beamed sub-image is converted into bin intensities using the depth and
intensity \textcolor{blue}{shader matrix parameters}. In a real imaging sonar, the echo
measured back is sampled
over time and the bin number is proportional to the sensor range. In other
words, the initial bins represent the closest distances, while the latest
bins are the farthest ones. Therefore a distance histogram (see Fig. \ref{fig:sonar_sim}(iii)) is computed in order to
group the sub-image pixels with the respective bins, according to the
depth \textcolor{blue}{shader matrix parameter and number of bins,
and calculate the accumulated intensity of each bin.}

Due to the acoustic beam spreading and absorption in the water, the final
bins have less echo strength than the first ones, because the energy is
twice lost, in the environment. To tackle with that issue, sonar devices
use an energy normalization based on time-varying gain for range dependence
compensation, which spreads losses in the bins. In our simulation approach,
the accumulated intensity in each bin (see Fig. \ref{fig:sonar_sim}(iv)) is normalized as

\begin{equation}
    \label{eq:1}
    I_{bin} = \sum\limits_{x=1}^N \frac{1}{N} \times S(i_{x}) \, ,
\end{equation}
where $I_{bin}$ is the intensity in the bin after energy normalization,
$x$ is the pixel location in the shader matrix, $N$ is the distance histogram
value (number of pixels) of that bin, $S(i_{x})$ is a sigmoid function,
and $i_{x}$ is the intensity value of the pixel $x$.

\textcolor{blue}{Finally, the sonar image resolution needs to be big enough to contain all
information of the bins. For that, the number of bins involved is directly
proportional to the sonar image resolution.}

% ----------------------------------------------------------------------------------

\subsection{Noise model}
\label{dev:noise}

Imaging sonar systems are disturbed by a multiplicative noise known as speckle,
which is caused by coherent processing of backscattered signals from multiple
distributed targets. \textcolor{blue}{This effect degrades} image quality and visual
evaluation. Speckle noise results in constructive and destructive interferences,
which are shown as bright and dark dots in the image. The noisy image has been
expressed as \cite{lee1980}:

\begin{equation}
\label{eq:2}
y(t) = x(t) \times n(t) \, ,
\end{equation}
where $t$ is the time instant, $y(t)$ is the noised image, $x(t)$ is the
free-noise image, $n(t)$ is the speckle noise matrix, and $\times$ symbol defines an
element-wise multiplication.

This type of noise is well-modeled as a Gaussian distribution (see Fig. \ref{fig:sonar_sim}(v)). The physical explanation is provided by the central limit theorem, which states that the
sum of many independent and identically distributed random variables tends
to behave as a Gaussian random variable \textcolor{blue}{\cite{papoulis2002}}. A Gaussian distribution is defined by following a non-uniform distribution, skewed towards low values, and applied as speckle noise in the simulated sonar image. \textcolor{blue}{This noise simulation is repeated for each virtual acoustic frame.}

\subsection{Integrating sonar device with Rock}
\label{dev:rock}

\textcolor{blue}{After the imaging sonar simulation process, from the virtual underwater scene to the degraded acoustic representation by noise, the resulting sonar data is encapsulated as Rock's sonar data type (see Fig. \ref{fig:sonar_sim}(vi)) and provided as I/O port of a Rock's component, allowing the interaction with other simulated models and applications.}

% ----------------------------------------------------------------------------------

\section{Simulation results and experimental analysis}
\label{results}

To evaluate our simulator, experiments were conducted by using a 3D model
of an AUV equipped with an MSIS and an FLS. Different scenarios were casted and studied. The sonar device configurations used were summarized in
Table \ref{table:sonar_settings}. In the experimental analysis, as the scene frames are being captured by
the sonars, the resulting acoustic images are sequentially presented,
on-the-fly \textcolor{blue}{(see Figs. \ref{fig:fls} and \ref{fig:msis})}.

\subsection{Experimental evaluation}

The virtual FLS from AUV was used to insonify the scenes from three distinct
scenarios. A docking station, in parallel with a pipeline on the seabed,
composes \textbf{the first scenario} (see Fig. \ref{fig:fls_scene1}); the
target surface is well-defined in the simulated acoustic frame (see
Fig. \ref{fig:fls_sim1}), even as the shadows and speckle noise; given that the docking station is metal-made, the texture and reflectivity were set, such
that a higher intensity shape was resulted in comparison with the other targets.
\textbf{The second scenario} presents the vehicle in front of a manifold model
in a non-uniform seabed (see Fig. \ref{fig:fls_scene2}); the target model was
insonified to generate the sonar frame from the underwater scene (see Fig. \ref{fig:fls_sim2}); the frontal
face of the target, as well the portion of the seabed and the degraded data
by noise, are clearly visible in the FLS image; also, a long acoustic shadow
is formed behind the manifold, occluding part of the scene. \textbf{The
third scenario} contains a sub-sea isolation valve (SSIV) structure, connected
to a pipeline in the bottom (see Fig. \ref{fig:fls_scene3}); the simulated
acoustic image, depicted in Fig. \ref{fig:fls_sim3}, also present shadows,
material properties and speckle noise effects. Due to sensor configuration and
robot position, the initial bins usually present a blind region in the three
simulated scenes, caused by absence of objects at lower ranges, similar to real
sonar images. It is noteworthy that the brightness of sea-floor decreases as the sea-floor is
farther from sonar, because of the normal orientation of the surface.

\begin{table}[t]
    \captionsetup{justification=justified}
    \caption{Sonar device configurations used on experimental evaluation.}
    \label{table:sonar_settings}
    \begin{center}
        \begin{tabular}{| c | c | c | c | c | c |}
            \hline
            \rule{0pt}{15pt}
            \makecell[c]{Device} & \makecell[c]{\shortstack{\# of\\ beams}} & \makecell[c]{\shortstack{\# of\\ bins}} & \makecell[c]{\shortstack{Field \\of view}} & \makecell[c]{\shortstack{Down\\tilt}} & \makecell{\shortstack{Motor\\Step}}\\
            \hline
            FLS  & 256 & 1000 & $120^{\circ}$ x $20^{\circ}$ & $20^{\circ}$  & - \\ \hline
            MSIS & 1   & 500  & $3^{\circ}$ x $35^{\circ}$	 & $0^{\circ}$  & $1.8^{\circ}$ \\ \hline
        \end{tabular}
    \end{center}
\end{table}

\begin{figure*}[!ht]
    \centering
    \setcounter{subfigure}{0}
    \subfigure[]{
        \includegraphics[width=0.4\paperwidth,height=6cm]{figs/fls_scene1}
        \label{fig:fls_scene1}
    }
    \setcounter{subfigure}{3}
    \subfigure[]{
        \includegraphics[width=0.4\paperwidth,height=6cm]{figs/fls_sim1}
        \label{fig:fls_sim1}
    }
    \setcounter{subfigure}{1}
    \subfigure[]{
        \includegraphics[width=0.4\paperwidth,height=6cm]{figs/fls_scene2}
        \label{fig:fls_scene2}
    }
    \setcounter{subfigure}{4}
    \subfigure[]{
        \includegraphics[width=0.4\paperwidth,height=6cm]{figs/fls_sim2}
        \label{fig:fls_sim2}
    }
    \setcounter{subfigure}{2}
    \subfigure[]{
        \includegraphics[width=0.4\paperwidth,height=6cm]{figs/fls_scene3}
        \label{fig:fls_scene3}
    }
    \setcounter{subfigure}{5}
    \subfigure[]{
        \includegraphics[width=0.4\paperwidth,height=6cm]{figs/fls_sim3}
        \label{fig:fls_sim3}
    }
    \captionsetup{justification=justified}
    \caption{Forward-looking sonar simulation experiments:
    \subref{fig:fls_scene1}, \subref{fig:fls_scene2} and \subref{fig:fls_scene3}
    present the virtual underwater trials, while \subref{fig:fls_sim1},
    \subref{fig:fls_sim2} and \subref{fig:fls_sim3} are the following acoustic
    representations of each scenario, respectively.}
    \label{fig:fls}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \setcounter{subfigure}{0}
    \subfigure[]{
        \includegraphics[width=0.4\paperwidth,height=6cm]{figs/msis_scene1}
        \label{fig:msis_scene1}
    }
    \setcounter{subfigure}{3}
    \subfigure[]{
        \includegraphics[width=0.4\paperwidth,height=6cm]{figs/msis_sim1}
        \label{fig:msis_sim1}
    }
    \setcounter{subfigure}{1}
    \subfigure[]{
        \includegraphics[width=0.4\paperwidth,height=6cm]{figs/msis_scene2}
        \label{fig:msis_scene2}
    }
    \setcounter{subfigure}{4}
    \subfigure[]{
        \includegraphics[width=0.4\paperwidth,height=6cm]{figs/msis_sim2}
        \label{fig:msis_sim2}
    }
    \setcounter{subfigure}{2}
    \subfigure[]{
        \includegraphics[width=0.4\paperwidth,height=6cm]{figs/msis_scene3}
        \label{fig:msis_scene3}
    }
    \setcounter{subfigure}{5}
    \subfigure[]{
        \includegraphics[width=0.4\paperwidth,height=6cm]{figs/msis_sim3}
        \label{fig:msis_sim3}
    }
    \captionsetup{justification=justified}
    \caption{Experiments using mechanical scanning imaging sonar in three
    different scenarios \subref{fig:msis_scene1}, \subref{fig:msis_scene2}
    and \subref{fig:msis_scene3}, and the respective processed simulated
    frames in horizontal axis in \subref{fig:msis_sim1} and
    \subref{fig:msis_sim2}, and vertical axis in \subref{fig:msis_sim3}.}
    \label{fig:msis}
\end{figure*}

\begin{table*}[t]
    \caption{Processing time to generate forward-looking sonar frames with different parameters.}
    \captionsetup{justification=justified}
    \label{table:fls}
    \begin{center}
        \begin{tabular}{| c | c | c | c | c | c | c |}
            \hline
            \# of samples & \# of beams & \# of bins & Field of view & Average time ($ms$) & Std dev ($ms$) & Frame rate ($fps$) \\
            \hline
            500     & 128     & 500       & $120^{\circ}$ x $20^{\circ}$        & 54.7    & 3.7   & 18.3 \\ \hline
            500     & 128     & 1000      & $120^{\circ}$ x $20^{\circ}$        & 72.3	& 8.9   & 13.8 \\ \hline
            500     & 256     & 500       & $120^{\circ}$ x $20^{\circ}$        & 198.7	& 17.1  & 5.0  \\ \hline
            500     & 256     & 1000      & $120^{\circ}$ x $20^{\circ}$        & 218.2	& 11.9  & 4.6  \\ \hline
            500     & 128     & 500       & $90^{\circ}$ x $15^{\circ}$         & 77.4	& 11.8  & 12.9 \\ \hline
            500     & 128     & 1000      & $90^{\circ}$ x $15^{\circ}$         & 94.6	& 10.2  & 10.6 \\ \hline
            500     & 256     & 500       & $90^{\circ}$ x $15^{\circ}$         & 260.8	& 18.5  & 3.8  \\ \hline
            500     & 256     & 1000      & $90^{\circ}$ x $15^{\circ}$         & 268.7	& 16.7  & 3.7  \\ \hline
        \end{tabular}
    \end{center}
\end{table*}

\begin{table*}
    \caption{Processing time to generate mechanical scanning imaging sonar samples with different parameters.}
    \captionsetup{justification=justified}
    \label{table:msis}
    \begin{center}
        \begin{tabular}{| c | c | c | c | c | c |}
            \hline
            \# of samples & \# of bins & Field of view & Average time ($ms$) & Std dev ($ms$) & Frame rate ($fps$) \\
            \hline
            500     & 500       & $3^{\circ}$ x $35^{\circ}$        & 8.8	    & 0.7  & 113.4 \\ \hline
            500     & 1000      & $3^{\circ}$ x $35^{\circ}$        & 34.5	& 1.6  & 29.0  \\ \hline
            500     & 500       & $2^{\circ}$ x $20^{\circ}$        & 10.3	& 0.6  & 96.7  \\ \hline
            500     & 1000      & $2^{\circ}$ x $20^{\circ}$        & 41.7	& 3.7  & 24.0  \\ \hline
        \end{tabular}
    \end{center}
\end{table*}

%%Luciano: Essa frase: "using an MSIS located in the back of the AUV with a vertical orientation, the scene was scanned to produce the acoustic visualization" - parece óbvia demais. Qual o sentido dela no final deste parágrafo?
% Rômulo: Os dois primeiros experimentos ocorreram com o MSIS montado horizontalmente, enquanto o último ele foi posicionado para leitura vertical
The MSIS was also simulated in three different experiments. The robot in a
big textured tank composes \textbf{the first scene} (see Fig.
\ref{fig:msis_scene1}); similar to the first scenario of FLS experiment,
the reflectivity and texture were set to the target; the rotation of the
sonar head position, by a complete $360^{\circ}$ scanning, produced the acoustic
frame of tank walls (see Fig. \ref{fig:msis_sim1}). \textbf{The second scene}
involves the vehicle's movement during the data acquisition process; the scene
contains a grid around the AUV (see Fig. \ref{fig:msis_scene2}), captured by a front MSIS mounted horizontally; this trial induces a distortion in the final
acoustic frame, because the relative sensor position with respect to the
surrounding object changes, as the sonar image is being built (see
Fig. \ref{fig:msis_sim2}); in this case, the robot rotates $20^{\circ}$ left
during the scanning. \textbf{The last scene} presents the AUV over oil
and gas structures on the sea bottom (see Fig. \ref{fig:msis_scene3});
using an MSIS located in the back of the AUV with a vertical orientation, the scene was scanned to produce the acoustic visualization; as illustrated in Fig. \ref{fig:msis_sim3}, object surfaces present clear definition in the slice scanning of the sea-floor.

All the experimental scenarios provided enough variability of specific phenomena usually found in real sonar images, such as acoustic shadows, noise interference, surface irregularities and properties, distortion during the acquisition process and variance of acoustic intensities. However, the speckle noise application is restricted to regions with acoustic intensity, as shown in Figs. \ref{fig:fls_sim3} and \ref{fig:msis_sim1}. This fact is due to our sonar model be multiplicative (defined in Eq. \ref{eq:2}). In real sonar images, the noise also granulates the shadows and blind regions. The sonar simulator can be improved by inserting an additive noise to our model.
\textcolor{blue}{The impact of additive noise on the image is more severe than that of multiplicative, and we decided to collect more data before including a specific additive noise in our simulator, at this moment.} A second feature missing in our simulated acoustic images are the ghost effects caused by reverberation. This lacking part can be addressed by implementation of a multi-path propagation model as found in \textcolor{blue}{\cite{huang2015b}. Simulating the multi-path reflection is computationally costly, thus we need to consider the real-time constraint by modeling and including the reverberation phenomenon.}

\subsection{Computational time}

Performance evaluation of the simulator was assessed by considering the suitability to run real-time applications. The experiments were performed on a laptop with Ubuntu 16.04 64 bits, Intel Core i7 3540M processor running at 3 GHz with 16GB DDR3 RAM memory and NVIDIA NVS 5200M video card. The elapsed time of each sonar data is stored to compute the average time, standard deviation and frame rate metrics, after $500$ iterations. The results found is summarized in Tables \ref{table:fls} and \ref{table:msis}. After changing the device parameters, such as number of bins, number of beams and field
of view, the proposed approach generated the sonar frames with a high
frame rate, for both sonar types, considering a real-world sonar. Given the Tritech Gemini 720i, a real forward-looking sonar sensor, with a field of view of $120^{\circ}$ by $20^{\circ}$ and 256 beams, presents a maximum update rate of 15 frames per second, the results allow the use of the sonar simulator for real-time applications. Also, the MSIS data used in the simulator is able to complete a $360^{\circ}$ scan sufficiently fast in comparison with a
real sonar as Tritech Micron DST. For the FLS device, these rates are superior to the rates lists by DeMarco \textit{et al} \cite{demarco2015} ($330 ms$) and Saç \textit{et al} \cite{sac2015} ($2.5 min$). \textcolor{blue}{For MSIS type, to the best of our knowledge, there is no previous work for comparison.}

%%%reescrever e colocar no local correto dentro das análises. Isso deve ficar em algum lugar lá em cima quando apresenta os resultados da tabela.
% RÔMULO: O que seria dentro das análises? conclusão?
According to previous results, since the number of bins is directly
proportional to sonar image resolution, as explained in Section
\ref{dev:sonardata}, this is also correlated with the computational time. When the number of bins increases, the simulator will have a bigger scene frame to compute, and generate the sonar data.

% \begin{figure}[t]
%     \includegraphics[width=\columnwidth]{figs/real_ssiv_03}
%     \centering
%     \captionsetup{justification=centering}
%     \caption{Image of a Tritech Gemini 720i Imaging Sonar of a mock-up of a sub-sea structure.}
%     \label{fig:real_sonar}
% \end{figure}

% \begin{table*}[t]
%     \caption{Processing time to generate forward-looking sonar frames with different parameters.}
%     \label{table:fls}
%     \begin{center}
%         \begin{tabular}{| c | c | c | c | c | c | c |}
%             \hline
%             \# of samples & \# of beams & \# of bins & Field of view & Average time ($ms$) & Std dev ($ms$) & Frame rate ($fps$) \\
%             \hline
%             500     & 128     & 500       & $120^{\circ}$ x $20^{\circ}$        & 54.7    & 3.7   & 18.3 \\ \hline
%             500     & 128     & 1000      & $120^{\circ}$ x $20^{\circ}$        & 72.3	& 8.9   & 13.8 \\ \hline
%             500     & 256     & 500       & $120^{\circ}$ x $20^{\circ}$        & 198.7	& 17.1  & 5.0  \\ \hline
%             500     & 256     & 1000      & $120^{\circ}$ x $20^{\circ}$        & 218.2	& 11.9  & 4.6  \\ \hline
%             500     & 128     & 500       & $90^{\circ}$ x $15^{\circ}$         & 77.4	& 11.8  & 12.9 \\ \hline
%             500     & 128     & 1000      & $90^{\circ}$ x $15^{\circ}$         & 94.6	& 10.2  & 10.6 \\ \hline
%             500     & 256     & 500       & $90^{\circ}$ x $15^{\circ}$         & 260.8	& 18.5  & 3.8  \\ \hline
%             500     & 256     & 1000      & $90^{\circ}$ x $15^{\circ}$         & 268.7	& 16.7  & 3.7  \\ \hline
%         \end{tabular}
%     \end{center}
% \end{table*}
%
% \begin{table*}
%     \caption{Processing time to generate mechanical scanning imaging sonar samples with different parameters.}
%     \label{table:msis}
%     \begin{center}
%         \begin{tabular}{| c | c | c | c | c | c |}
%             \hline
%             \# of samples & \# of bins & Field of view & Average time ($ms$) & Std dev ($ms$) & Frame rate ($fps$) \\
%             \hline
%             500     & 500       & $3^{\circ}$ x $35^{\circ}$        & 8.8	    & 0.7  & 113.4 \\ \hline
%             500     & 1000      & $3^{\circ}$ x $35^{\circ}$        & 34.5	& 1.6  & 29.0  \\ \hline
%             500     & 500       & $2^{\circ}$ x $20^{\circ}$        & 10.3	& 0.6  & 96.7  \\ \hline
%             500     & 1000      & $2^{\circ}$ x $20^{\circ}$        & 41.7	& 3.7  & 24.0  \\ \hline
%         \end{tabular}
%     \end{center}
% \end{table*}

% ----------------------------------------------------------------------------------

\section{Conclusion and future work}
\label{conclusion}

\textcolor{blue}{A GPU-based simulator for imaging sonar simulation is presented here. The system is able to reproduce the operation mode of two different types of sonar devices (FLS and MSIS) in real-time. The real sonar image singularities, such as multiplicative noise, surface properties and acoustic shadows are addressed and represented in the simulated frames. Specially for the shadows, the acoustic representation can present information as useful as the insonified object. Considering the qualitative results, the sonar simulator can be used by feature detection algorithms, based on corners, lines and shapes. Also, the computation time to build one sonar frame was calculated using different device settings. The vertex and fragment processing during the underwater scene rendering accelerates the sonar image building, providing an average time close or better than real imaging devices. These results allowed the use of this imaging sonar simulator by real-time applications, such as obstacle detection and avoidance, and object tracking. We are analyzing now a way to add the reverberation effect to perform a more realistic sensing, as well as, without significantly effect the computational time must. We are working now on how to include an additive noise in the simulation of the acoustic images.}

%%%RÔMULO: Não faz muito sentido incluir isso aqui. Acho melhor tirar.
%\textcolor{blue}{Recently we had the opportunity to record a lot of imaging sonar data during tests. The target structures we were operating on, have been made by us ourselves and we have detailed models in simulation (see Figs. \ref{fig:fls_sim3} and \ref{fig:real_sonar}). We therefore plan to use this data in the  next months to analyse the quality of the simulation.}


% \nocite{*}
\bibliographystyle{model3-num-names}
\bibliography{elsarticle-template-3-num}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model3-num-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}


\end{document}
